{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f8a51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from turtle import forward\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from scipy.fft import fft\n",
    "\n",
    "from pathlib import Path\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms  \n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "def acc_m(test_dataset, model):\n",
    "    device = next(model.parameters()).device\n",
    "    y_test = []\n",
    "    y_test_hat = []\n",
    "    model.eval()\n",
    "    \n",
    "    for inputs, labels in test_dataset:\n",
    "        # Add batch dimension like in training\n",
    "        inputs = inputs.unsqueeze(0)  # [freq, time] -> [1, freq, time]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs.to(device))\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        \n",
    "        y_test.append(labels)  # labels is already an int\n",
    "        y_test_hat.extend(preds)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_test_hat)\n",
    "    print(f'Accuracy: {acc*100:.2f} %')\n",
    "    cm = confusion_matrix(y_test, y_test_hat)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8860679",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioFolder(Dataset):\n",
    "    def __init__(self, root, transform=None, target_transform=None,\n",
    "                 extensions=(\".wav\", \".flac\", \".mp3\"), allowed_classes=None):\n",
    "        self.root = Path(root)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.extensions = extensions\n",
    "        self.allowed_classes = allowed_classes\n",
    "\n",
    "        # crawl sub-dirs and record (path, class_idx)\n",
    "        all_classes = sorted({p.parent.name for p in self.root.rglob(\"*\") if p.suffix in extensions})\n",
    "        \n",
    "        # Filter classes if allowed_classes is provided\n",
    "        if allowed_classes is not None:\n",
    "            self.classes = [c for c in all_classes if c in allowed_classes]\n",
    "        else:\n",
    "            self.classes = all_classes\n",
    "            \n",
    "        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
    "        \n",
    "        # Only include samples from allowed classes\n",
    "        self.samples = [(p, self.class_to_idx[p.parent.name])\n",
    "                        for p in self.root.rglob(\"*\") \n",
    "                        if p.suffix in extensions and p.parent.name in self.classes]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        waveform, sr = torchaudio.load(path, backend=\"ffmpeg\")\n",
    "\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return waveform[0], label\n",
    "\n",
    "# TODO - it seems it introduces randomness here\n",
    "transform_wav = lambda x: torchaudio.functional.spectrogram(\n",
    "        x,\n",
    "        pad=0,\n",
    "        win_length = 255,\n",
    "        window=torch.hann_window(255),  # Hann window\n",
    "        n_fft=255,\n",
    "        normalized=False,\n",
    "        center=False ,\n",
    "        hop_length=128,                  # overlap = n_fft - hop_length\n",
    "        power=None                       # None -> keep complex, 2.0 -> magnitude^2\n",
    "    )\n",
    "\n",
    "ALLOWED_CLASSES = ['normal', 'murmur', 'extrahls', 'artifact']\n",
    "dataset_wav = AudioFolder('../data/hb/', transform_wav, allowed_classes=ALLOWED_CLASSES)\n",
    "dataset_wav_train, dataset_wav_test = train_test_split(dataset_wav, train_size=0.9)\n",
    "\n",
    "\n",
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, n_classes, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        def build_Seq():\n",
    "            return nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            # nn.BatchNorm2d(16),\n",
    "            nn.Conv2d(32, 8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        self.real = build_Seq()\n",
    "\n",
    "        self.imag = build_Seq()\n",
    "\n",
    "        in_size = 24000 * 2\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_size, in_size // 8),\n",
    "            nn.Linear(in_size // 8, in_size // 32),\n",
    "            nn.Linear(in_size // 32, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        real_input = x.real.unsqueeze(1)\n",
    "        img_input = x.imag.unsqueeze(1)\n",
    "\n",
    "        out_l = self.real(real_input)   \n",
    "        out_r = self.imag(img_input)        \n",
    "\n",
    "        out_l = F.adaptive_avg_pool2d(out_l, (30, 100))  # Fixed reasonable size\n",
    "        out_r = F.adaptive_avg_pool2d(out_r, (30, 100))  # [batch, 8, 30, 100]\n",
    "\n",
    "        out_l = out_l.flatten(1)  # [batch, 24000]\n",
    "        out_r = out_r.flatten(1)  # [batch, 24000]\n",
    "        out = torch.cat([out_l, out_r], dim=1)  # [batch, 2*channels*freq*time]\n",
    "        return self.fc(out)\n",
    "    \n",
    "\n",
    "class AudioClassifier2(nn.Module):\n",
    "    def __init__(self, n_classes, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        def build_Seq():\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(1, 6, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2,2),\n",
    "                nn.BatchNorm2d(6),\n",
    "                nn.Conv2d(6, 16, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2,2),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.AdaptiveAvgPool2d((16, 32)),  # [batch, 8, 30, 100]\n",
    "                nn.Flatten(1)\n",
    "            )\n",
    "        self.real = build_Seq()\n",
    "\n",
    "        self.imag = build_Seq()\n",
    "\n",
    "        in_size = 16384\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_size, 128),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Linear(64, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.real.shape)\n",
    "        real_input = x.real.unsqueeze(1)\n",
    "        img_input = x.imag.unsqueeze(1)\n",
    "        # real_input = x.real\n",
    "        # img_input = x.imag     \n",
    "\n",
    "        out_l = self.real(real_input)   \n",
    "        out_r = self.imag(img_input)        \n",
    "\n",
    "        out = torch.cat([out_l, out_r], dim=1)  # [batch, 2*channels*freq*time]\n",
    "        # print(out.shape)\n",
    "        return self.fc(out)\n",
    "    \n",
    "    \n",
    "ac = AudioClassifier(len(dataset_wav.classes))\n",
    "res_t = ac(dataset_wav_test[0][0].unsqueeze(0))\n",
    "res_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74deb004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "data_path = '../data/hb/'\n",
    "\n",
    "for i in range(10):\n",
    "    unl = 'unlabelled'\n",
    "    unlab = Path(data_path,unl)\n",
    "    random_unlabel = random.choice(list(unlab.glob('*')))\n",
    "    waveform, sr = torchaudio.load(random_unlabel, backend=\"ffmpeg\")\n",
    "    transformed = fft(waveform.reshape(-1).numpy())\n",
    "    print(transformed.shape)\n",
    "# STFT: windowing + overlap + FFT all in one\n",
    "    spec = torchaudio.functional.spectrogram(\n",
    "        waveform,\n",
    "        pad=0,\n",
    "        win_length = 255,\n",
    "        window=torch.hann_window(255),  # Hann window\n",
    "        n_fft=255,\n",
    "        normalized=False,\n",
    "        center=False ,\n",
    "        hop_length=128,                  # overlap = n_fft - hop_length\n",
    "        power=None                       # None -> keep complex, 2.0 -> magnitude^2\n",
    "    )\n",
    "    print(spec.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c78196",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = dataset_wav_test.pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea94604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm = sm()\n",
    "sm[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c711b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = dataset_wav_train\n",
    "test_dataset = dataset_wav_test\n",
    "\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "device = 'cuda'\n",
    "ac = ac.to(device)\n",
    "optimiser = torch.optim.Adam(ac.parameters(), lr=0.0001)\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (inputs, labels) in enumerate(train_dataset):\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        # Add batch dimension manually\n",
    "        inputs = inputs.unsqueeze(0)  # [1, freq, time]\n",
    "        labels = torch.tensor([labels])  # [1]\n",
    "        \n",
    "        y_pred = ac(inputs.to(device))\n",
    "        loss_v = loss(y_pred, labels.to(device))\n",
    "        \n",
    "        loss_v.backward()\n",
    "        losses.append(loss_v.item())\n",
    "        optimiser.step()\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(f'Epoch {epoch}/{NUM_EPOCHS}, Sample {i}, Loss: {loss_v.item():.4f}')\n",
    "    \n",
    "    print(f'Completed Epoch {epoch+1}')\n",
    "    acc_m(test_dataset, ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c70ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 1 # should've convert that into spectogram\n",
    "trainloader = DataLoader(dataset_wav_train, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "testloader = DataLoader(dataset_wav_test, batch_size=batch_size, shuffle=False,num_workers=8)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "device = 'cuda'\n",
    "ac = ac.to(device)\n",
    "optimiser = torch.optim.Adam(ac.parameters())\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        \n",
    "        y_pred = ac(inputs.to(device))\n",
    "        loss_v = loss(y_pred, labels.to(device))\n",
    "        \n",
    "        loss_v.backward()\n",
    "        losses.append(loss_v.item())\n",
    "        optimiser.step()\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(f'Epoch {epoch}/{NUM_EPOCHS}, Sample {i}, Loss: {loss_v.item():.4f}')\n",
    "    \n",
    "    print(f'Completed Epoch {epoch+1}')\n",
    "    ac.eval()\n",
    "    with torch.no_grad() :            \n",
    "        y_real = []\n",
    "        y_preds = []\n",
    "        for i, (inputs, labels) in enumerate(testloader, 0):\n",
    "            y_real.append(labels)\n",
    "            y_preds.append(torch.argmax(ac(inputs.to(device)), dim=1).cpu().numpy())\n",
    "        y_real = np.asarray(y_real).flatten()\n",
    "        y_preds = np.asarray(y_preds).flatten()\n",
    "        cm = confusion_matrix(y_real, y_preds)\n",
    "\n",
    "        print(\"\\nPer-class accuracy:\")\n",
    "        for i, cls in enumerate(dataset_wav.classes):\n",
    "            class_acc = cm[i,i] / cm[i].sum() if cm[i].sum() > 0 else 0\n",
    "            print(f\"{cls}: {class_acc*100:.1f}% ({cm[i,i]}/{cm[i].sum()})\")\n",
    "\n",
    "        # Total accuracy\n",
    "        total_acc = cm.diagonal().sum() / cm.sum()\n",
    "        print(f\"Total Accuracy: {total_acc*100:.2f}%\")            \n",
    "    ac.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ef34f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_class_distribution(data_subset, original_dataset, name=\"Dataset\"):\n",
    "    class_counts = {}\n",
    "    for _, label in data_subset:\n",
    "        class_counts[label] = class_counts.get(label, 0) + 1\n",
    "    \n",
    "    print(f\"\\n{name} - Class Distribution:\")\n",
    "    print(f\"{'Class':<15} {'Count':<10} {'Percentage':<10}\")\n",
    "    print(\"-\" * 35)\n",
    "    total = len(data_subset)\n",
    "    for class_name in original_dataset.classes:\n",
    "        class_idx = original_dataset.class_to_idx[class_name]\n",
    "        count = class_counts.get(class_idx, 0)\n",
    "        pct = (count / total * 100) if total > 0 else 0\n",
    "        print(f\"{class_name:<15} {count:<10} {pct:>6.1f}%\")\n",
    "    print(f\"{'Total':<15} {total:<10}\")\n",
    "\n",
    "# Use original dataset for class info\n",
    "print_class_distribution(dataset_wav_train, dataset_wav, \"Training Set\")\n",
    "print_class_distribution(dataset_wav_test, dataset_wav, \"Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e5a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Get all labels for stratification\n",
    "all_labels = [label for _, label in dataset_wav]\n",
    "\n",
    "# Cross-validation loop\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(range(len(dataset_wav)), all_labels)):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"FOLD {fold + 1}/5\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Create subsets for this fold\n",
    "    train_subset = Subset(dataset_wav, train_idx)\n",
    "    test_subset = Subset(dataset_wav, test_idx)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    trainloader = DataLoader(train_subset, batch_size=1, shuffle=True, num_workers=8)\n",
    "    testloader = DataLoader(test_subset, batch_size=1, shuffle=False, num_workers=8)\n",
    "    \n",
    "    # Fresh model for each fold\n",
    "    ac = AudioClassifier2(len(dataset_wav.classes)).to('cuda')\n",
    "    optimiser = torch.optim.Adam(ac.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(20):  # Fewer epochs per fold\n",
    "        ac.train()\n",
    "        for i, (inputs, labels) in enumerate(trainloader):\n",
    "            optimiser.zero_grad()\n",
    "            y_pred = ac(inputs.to('cuda'))\n",
    "            loss_v = loss(y_pred, labels.to('cuda'))\n",
    "            loss_v.backward()\n",
    "            optimiser.step()\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Fold {fold+1}, Epoch {epoch}, Loss: {loss_v.item():.4f}\")\n",
    "    \n",
    "    # Evaluate this fold\n",
    "    ac.eval()\n",
    "    y_real, y_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            outputs = ac(inputs.to('cuda'))\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            y_real.extend(labels.numpy())\n",
    "            y_preds.extend(preds)\n",
    "    \n",
    "    acc = (np.array(y_real) == np.array(y_preds)).mean()\n",
    "    fold_accuracies.append(acc)\n",
    "    print(f\"Fold {fold+1} Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nAverage Accuracy: {np.mean(fold_accuracies)*100:.2f}% (+/- {np.std(fold_accuracies)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c8beeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "Counter(y_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fabc85a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
