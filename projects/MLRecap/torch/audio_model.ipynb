{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a8f8a51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from turtle import forward\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from scipy.fft import fft\n",
    "\n",
    "from pathlib import Path\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms  \n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "def acc_m(test_dataset, model):\n",
    "    device = next(model.parameters()).device\n",
    "    y_test = []\n",
    "    y_test_hat = []\n",
    "    model.eval()\n",
    "    \n",
    "    for inputs, labels in test_dataset:\n",
    "        # Add batch dimension like in training\n",
    "        inputs = inputs.unsqueeze(0)  # [freq, time] -> [1, freq, time]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs.to(device))\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        \n",
    "        y_test.append(labels)  # labels is already an int\n",
    "        y_test_hat.extend(preds)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_test_hat)\n",
    "    print(f'Accuracy: {acc*100:.2f} %')\n",
    "    cm = confusion_matrix(y_test, y_test_hat)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c8860679",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AudioFolder(Dataset):\n",
    "    def __init__(self, root, transform=None, target_transform=None,\n",
    "                 extensions=(\".wav\", \".flac\", \".mp3\")):\n",
    "        self.root = Path(root)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.extensions = extensions\n",
    "\n",
    "        # crawl sub-dirs and record (path, class_idx)\n",
    "        self.classes = sorted({p.parent.name for p in self.root.rglob(\"*\") if p.suffix in extensions})\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
    "        self.samples = [(p, self.class_to_idx[p.parent.name])\n",
    "                        for p in self.root.rglob(\"*\") if p.suffix in extensions]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        waveform, sr = torchaudio.load(path, backend=\"ffmpeg\")\n",
    "\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return waveform[0], label\n",
    "\n",
    "\n",
    "transform_wav = lambda x: torchaudio.functional.spectrogram(\n",
    "        x,\n",
    "        pad=0,\n",
    "        win_length = 255,\n",
    "        window=torch.hann_window(255),  # Hann window\n",
    "        n_fft=255,\n",
    "        normalized=False,\n",
    "        center=False ,\n",
    "        hop_length=128,                  # overlap = n_fft - hop_length\n",
    "        power=None                       # None -> keep complex, 2.0 -> magnitude^2\n",
    "    )\n",
    "dataset_wav = AudioFolder('../data/hb/', transform_wav)\n",
    "dataset_wav_train, dataset_wav_test = train_test_split(dataset_wav, train_size=0.9)\n",
    "\n",
    "\n",
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, n_classes, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        def build_Seq():\n",
    "            return nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            # nn.BatchNorm2d(16),\n",
    "            nn.Conv2d(32, 8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        self.real = build_Seq()\n",
    "\n",
    "        self.imag = build_Seq()\n",
    "\n",
    "        in_size = 24000 * 2\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_size, in_size // 8),\n",
    "            nn.Linear(in_size // 8, in_size // 32),\n",
    "            nn.Linear(in_size // 32, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        real_input = x.real.unsqueeze(1)\n",
    "        img_input = x.imag.unsqueeze(1)\n",
    "\n",
    "        out_l = self.real(real_input)   \n",
    "        out_r = self.imag(img_input)        \n",
    "\n",
    "        out_l = F.adaptive_avg_pool2d(out_l, (30, 100))  # Fixed reasonable size\n",
    "        out_r = F.adaptive_avg_pool2d(out_r, (30, 100))  # [batch, 8, 30, 100]\n",
    "\n",
    "        out_l = out_l.flatten(1)  # [batch, 24000]\n",
    "        out_r = out_r.flatten(1)  # [batch, 24000]\n",
    "        out = torch.cat([out_l, out_r], dim=1)  # [batch, 2*channels*freq*time]\n",
    "        return self.fc(out)\n",
    "    \n",
    "ac = AudioClassifier(len(dataset_wav.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "74deb004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41625,)\n",
      "torch.Size([1, 128, 324])\n",
      "(48057,)\n",
      "torch.Size([1, 128, 374])\n",
      "(49400,)\n",
      "torch.Size([1, 128, 384])\n",
      "(47952,)\n",
      "torch.Size([1, 128, 373])\n",
      "(47952,)\n",
      "torch.Size([1, 128, 373])\n",
      "(60968,)\n",
      "torch.Size([1, 128, 475])\n",
      "(48057,)\n",
      "torch.Size([1, 128, 374])\n",
      "(50145,)\n",
      "torch.Size([1, 128, 390])\n",
      "(47092,)\n",
      "torch.Size([1, 128, 366])\n",
      "(47092,)\n",
      "torch.Size([1, 128, 366])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "data_path = '../data/hb/'\n",
    "\n",
    "for i in range(10):\n",
    "    unl = 'unlabelled'\n",
    "    unlab = Path(data_path,unl)\n",
    "    random_unlabel = random.choice(list(unlab.glob('*')))\n",
    "    waveform, sr = torchaudio.load(random_unlabel, backend=\"ffmpeg\")\n",
    "    transformed = fft(waveform.reshape(-1).numpy())\n",
    "    print(transformed.shape)\n",
    "# STFT: windowing + overlap + FFT all in one\n",
    "    spec = torchaudio.functional.spectrogram(\n",
    "        waveform,\n",
    "        pad=0,\n",
    "        win_length = 255,\n",
    "        window=torch.hann_window(255),  # Hann window\n",
    "        n_fft=255,\n",
    "        normalized=False,\n",
    "        center=False ,\n",
    "        hop_length=128,                  # overlap = n_fft - hop_length\n",
    "        power=None                       # None -> keep complex, 2.0 -> magnitude^2\n",
    "    )\n",
    "    print(spec.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d2c78196",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = dataset_wav_test.pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4ea94604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 428])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sm = sm()\n",
    "sm[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3c711b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/20, Sample 0, Loss: 1.7950\n",
      "Epoch 0/20, Sample 50, Loss: 0.8155\n",
      "Epoch 0/20, Sample 100, Loss: 1.5056\n",
      "Epoch 0/20, Sample 150, Loss: 1.7920\n",
      "Completed Epoch 1\n",
      "Accuracy: 16.67 %\n",
      "[[0 0 0 2 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 7 0]\n",
      " [0 0 0 3 0]\n",
      " [0 0 0 5 0]]\n",
      "Epoch 1/20, Sample 0, Loss: 1.6342\n",
      "Epoch 1/20, Sample 50, Loss: 1.6106\n",
      "Epoch 1/20, Sample 100, Loss: 1.6471\n",
      "Epoch 1/20, Sample 150, Loss: 1.8081\n",
      "Completed Epoch 2\n",
      "Accuracy: 16.67 %\n",
      "[[0 0 0 2 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 7 0]\n",
      " [0 0 0 3 0]\n",
      " [0 0 0 5 0]]\n",
      "Epoch 2/20, Sample 0, Loss: 1.5690\n",
      "Epoch 2/20, Sample 50, Loss: 1.4755\n",
      "Epoch 2/20, Sample 100, Loss: 1.6046\n",
      "Epoch 2/20, Sample 150, Loss: 1.7318\n",
      "Completed Epoch 3\n",
      "Accuracy: 16.67 %\n",
      "[[0 0 0 2 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 7 0]\n",
      " [0 0 0 3 0]\n",
      " [0 0 0 5 0]]\n",
      "Epoch 3/20, Sample 0, Loss: 1.5456\n",
      "Epoch 3/20, Sample 50, Loss: 1.3942\n",
      "Epoch 3/20, Sample 100, Loss: 1.2745\n",
      "Epoch 3/20, Sample 150, Loss: 1.9595\n",
      "Completed Epoch 4\n",
      "Accuracy: 27.78 %\n",
      "[[0 0 0 0 2]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 0 7]\n",
      " [0 0 0 0 3]\n",
      " [0 0 0 0 5]]\n",
      "Epoch 4/20, Sample 0, Loss: 1.0679\n",
      "Epoch 4/20, Sample 50, Loss: 0.2078\n",
      "Epoch 4/20, Sample 100, Loss: 0.0712\n",
      "Epoch 4/20, Sample 150, Loss: 0.6004\n",
      "Completed Epoch 5\n",
      "Accuracy: 44.44 %\n",
      "[[1 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 7 0 0]\n",
      " [2 0 1 0 0]\n",
      " [0 0 5 0 0]]\n",
      "Epoch 5/20, Sample 0, Loss: 0.1482\n",
      "Epoch 5/20, Sample 50, Loss: 0.0029\n",
      "Epoch 5/20, Sample 100, Loss: 0.0025\n",
      "Epoch 5/20, Sample 150, Loss: 0.0073\n",
      "Completed Epoch 6\n",
      "Accuracy: 22.22 %\n",
      "[[0 0 0 2 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 2 5 0]\n",
      " [0 0 1 2 0]\n",
      " [0 0 1 4 0]]\n",
      "Epoch 6/20, Sample 0, Loss: 0.0127\n",
      "Epoch 6/20, Sample 50, Loss: 0.0012\n",
      "Epoch 6/20, Sample 100, Loss: 0.0008\n",
      "Epoch 6/20, Sample 150, Loss: 0.0023\n",
      "Completed Epoch 7\n",
      "Accuracy: 16.67 %\n",
      "[[1 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 2 5 0]\n",
      " [2 0 0 0 1]\n",
      " [4 0 0 1 0]]\n",
      "Epoch 7/20, Sample 0, Loss: 0.0061\n",
      "Epoch 7/20, Sample 50, Loss: 0.0163\n",
      "Epoch 7/20, Sample 100, Loss: 0.0101\n",
      "Epoch 7/20, Sample 150, Loss: 0.0009\n",
      "Completed Epoch 8\n",
      "Accuracy: 16.67 %\n",
      "[[1 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 2 5 0]\n",
      " [2 0 0 0 1]\n",
      " [4 0 0 1 0]]\n",
      "Epoch 8/20, Sample 0, Loss: 0.0010\n",
      "Epoch 8/20, Sample 50, Loss: 0.0032\n",
      "Epoch 8/20, Sample 100, Loss: 0.0036\n",
      "Epoch 8/20, Sample 150, Loss: 0.0005\n",
      "Completed Epoch 9\n",
      "Accuracy: 16.67 %\n",
      "[[1 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [3 0 2 2 0]\n",
      " [2 0 0 0 1]\n",
      " [4 0 0 1 0]]\n",
      "Epoch 9/20, Sample 0, Loss: 0.0005\n",
      "Epoch 9/20, Sample 50, Loss: 0.0014\n",
      "Epoch 9/20, Sample 100, Loss: 0.0022\n",
      "Epoch 9/20, Sample 150, Loss: 0.0003\n",
      "Completed Epoch 10\n",
      "Accuracy: 16.67 %\n",
      "[[1 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [3 0 2 2 0]\n",
      " [2 0 0 0 1]\n",
      " [5 0 0 0 0]]\n",
      "Epoch 10/20, Sample 0, Loss: 0.0004\n",
      "Epoch 10/20, Sample 50, Loss: 0.0009\n",
      "Epoch 10/20, Sample 100, Loss: 0.0015\n",
      "Epoch 10/20, Sample 150, Loss: 0.0002\n",
      "Completed Epoch 11\n",
      "Accuracy: 16.67 %\n",
      "[[1 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [3 0 2 2 0]\n",
      " [2 0 0 0 1]\n",
      " [5 0 0 0 0]]\n",
      "Epoch 11/20, Sample 0, Loss: 0.0003\n",
      "Epoch 11/20, Sample 50, Loss: 0.0006\n",
      "Epoch 11/20, Sample 100, Loss: 0.0011\n",
      "Epoch 11/20, Sample 150, Loss: 0.0001\n",
      "Completed Epoch 12\n",
      "Accuracy: 16.67 %\n",
      "[[1 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [3 0 2 2 0]\n",
      " [2 0 0 0 1]\n",
      " [5 0 0 0 0]]\n",
      "Epoch 12/20, Sample 0, Loss: 0.0002\n",
      "Epoch 12/20, Sample 50, Loss: 0.0005\n",
      "Epoch 12/20, Sample 100, Loss: 0.0008\n",
      "Epoch 12/20, Sample 150, Loss: 0.0001\n",
      "Completed Epoch 13\n",
      "Accuracy: 16.67 %\n",
      "[[1 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [3 0 2 2 0]\n",
      " [2 0 0 0 1]\n",
      " [4 0 1 0 0]]\n",
      "Epoch 13/20, Sample 0, Loss: 0.0002\n",
      "Epoch 13/20, Sample 50, Loss: 0.0004\n",
      "Epoch 13/20, Sample 100, Loss: 0.0006\n",
      "Epoch 13/20, Sample 150, Loss: 0.0001\n",
      "Completed Epoch 14\n",
      "Accuracy: 16.67 %\n",
      "[[1 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [3 0 2 2 0]\n",
      " [2 0 0 0 1]\n",
      " [4 0 1 0 0]]\n",
      "Epoch 14/20, Sample 0, Loss: 0.0001\n",
      "Epoch 14/20, Sample 50, Loss: 0.0003\n",
      "Epoch 14/20, Sample 100, Loss: 0.0005\n",
      "Epoch 14/20, Sample 150, Loss: 0.0001\n",
      "Completed Epoch 15\n",
      "Accuracy: 16.67 %\n",
      "[[1 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [3 0 2 2 0]\n",
      " [2 0 0 0 1]\n",
      " [4 0 1 0 0]]\n",
      "Epoch 15/20, Sample 0, Loss: 0.0001\n",
      "Epoch 15/20, Sample 50, Loss: 0.0002\n",
      "Epoch 15/20, Sample 100, Loss: 0.0004\n",
      "Epoch 15/20, Sample 150, Loss: 0.0001\n",
      "Completed Epoch 16\n",
      "Accuracy: 16.67 %\n",
      "[[1 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [3 0 2 2 0]\n",
      " [2 0 0 0 1]\n",
      " [3 0 1 1 0]]\n",
      "Epoch 16/20, Sample 0, Loss: 0.0001\n",
      "Epoch 16/20, Sample 50, Loss: 0.0002\n",
      "Epoch 16/20, Sample 100, Loss: 0.0003\n",
      "Epoch 16/20, Sample 150, Loss: 0.0001\n",
      "Completed Epoch 17\n",
      "Accuracy: 16.67 %\n",
      "[[1 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [3 0 2 2 0]\n",
      " [2 0 0 0 1]\n",
      " [3 0 1 1 0]]\n",
      "Epoch 17/20, Sample 0, Loss: 0.0001\n",
      "Epoch 17/20, Sample 50, Loss: 0.0002\n",
      "Epoch 17/20, Sample 100, Loss: 0.0003\n",
      "Epoch 17/20, Sample 150, Loss: 0.0000\n",
      "Completed Epoch 18\n",
      "Accuracy: 16.67 %\n",
      "[[1 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [3 0 2 2 0]\n",
      " [2 0 0 0 1]\n",
      " [3 0 1 1 0]]\n",
      "Epoch 18/20, Sample 0, Loss: 0.0001\n",
      "Epoch 18/20, Sample 50, Loss: 0.0001\n",
      "Epoch 18/20, Sample 100, Loss: 0.0002\n",
      "Epoch 18/20, Sample 150, Loss: 0.0000\n",
      "Completed Epoch 19\n",
      "Accuracy: 16.67 %\n",
      "[[1 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [3 0 2 2 0]\n",
      " [2 0 0 0 1]\n",
      " [3 0 1 1 0]]\n",
      "Epoch 19/20, Sample 0, Loss: 0.0001\n",
      "Epoch 19/20, Sample 50, Loss: 0.0001\n",
      "Epoch 19/20, Sample 100, Loss: 0.0002\n",
      "Epoch 19/20, Sample 150, Loss: 0.0000\n",
      "Completed Epoch 20\n",
      "Accuracy: 16.67 %\n",
      "[[1 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [3 0 2 2 0]\n",
      " [2 0 0 0 1]\n",
      " [3 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = dataset_wav_train\n",
    "test_dataset = dataset_wav_test\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "device = 'cuda'\n",
    "ac = ac.to(device)\n",
    "optimiser = torch.optim.Adam(ac.parameters(), lr=0.0001)\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (inputs, labels) in enumerate(train_dataset):\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        # Add batch dimension manually\n",
    "        inputs = inputs.unsqueeze(0)  # [1, freq, time]\n",
    "        labels = torch.tensor([labels])  # [1]\n",
    "        \n",
    "        y_pred = ac(inputs.to(device))\n",
    "        loss_v = loss(y_pred, labels.to(device))\n",
    "        \n",
    "        loss_v.backward()\n",
    "        losses.append(loss_v.item())\n",
    "        optimiser.step()\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(f'Epoch {epoch}/{NUM_EPOCHS}, Sample {i}, Loss: {loss_v.item():.4f}')\n",
    "    \n",
    "    print(f'Completed Epoch {epoch+1}')\n",
    "    acc_m(test_dataset, ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c70ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ef34f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e5a80a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4483ff3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
