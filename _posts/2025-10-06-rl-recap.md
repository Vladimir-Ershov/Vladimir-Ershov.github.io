---
layout: post
title: "Reinforcement Learning: Interview Prep Guide"
date: 2025-10-06
categories: [machine-learning, reinforcement-learning]
tags: [ai-generated, interview-prep]
description: "A comprehensive guide to RL concepts for interviews, with focus on connections to LLMs and RLHF. Includes practical PyTorch implementations and detailed explanations."
---

## Key Conceptual Differences from Standard ML

**Paradigm shift:**
- **Supervised ML**: Learn from labeled examples (input → output mapping)
- **RL**: Learn from experience through trial-and-error, optimizing cumulative rewards over time
- No ground truth labels; agent discovers optimal behavior by interacting with environment

## Core RL Concepts (Interview Questions)

**Fundamental Framework:**
- **Agent, Environment, State, Action, Reward** - explain the interaction loop
- **Policy (π)**: What the agent does - mapping from states to actions
- **Value function (V, Q)**: Expected cumulative reward from a state/state-action pair
- **Exploration vs Exploitation**: Balance trying new actions vs. using known good ones

**Key Algorithms You Should Know:**
- **Q-Learning**: Off-policy, learns optimal Q-values using Bellman equation
- **Policy Gradients**: Directly optimize policy parameters (important for LLMs!)
- **Actor-Critic**: Combines value and policy methods
- **PPO (Proximal Policy Optimization)**: Standard for RLHF in LLMs

## Implementation Steps

1. **Define the MDP**: States, actions, transition dynamics, reward function
2. **Choose algorithm**: Value-based (Q-learning) vs. policy-based (PG)
3. **Initialize**: Policy/value function (often neural networks)
4. **Training loop**:
   - Agent takes action in environment
   - Observe next state and reward
   - Update policy/value function
   - Repeat until convergence
5. **Evaluation**: Test learned policy without exploration

## Connection to Your LLM Experience

**RLHF (RL from Human Feedback)** - likely interview topic:
- Pre-train LLM on text (supervised)
- Collect human preference data on outputs
- Train reward model to predict preferences
- Use PPO to fine-tune LLM policy to maximize predicted rewards
- This is how ChatGPT-style models align with human preferences

**Key differences in LLM context:**
- State: conversation history/prompt
- Action: next token generation
- Reward: human preference scores or safety metrics

## Common Interview Questions

- Explain the difference between on-policy and off-policy learning
- What's the credit assignment problem?
- How do you handle sparse rewards?
- Explain the Bellman equation
- What's the difference between model-based and model-free RL?
- How would you apply RL to [specific problem]?
- Explain temporal difference learning
- What are the challenges in RL (sample efficiency, stability, etc.)?

# Detailed RL Answers

## 1. Exploration vs Exploitation in PyTorch

**Epsilon-Greedy (most common):**
```python
import torch
import random

class EpsilonGreedy:
    def __init__(self, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
    
    def select_action(self, q_values):
        if random.random() < self.epsilon:
            # Explore: random action
            return random.randint(0, len(q_values) - 1)
        else:
            # Exploit: best action
            return torch.argmax(q_values).item()
    
    def decay(self):
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)
```

**Boltzmann/Softmax exploration:**
```python
def softmax_action(q_values, temperature=1.0):
    probs = torch.softmax(q_values / temperature, dim=0)
    return torch.multinomial(probs, 1).item()
```

---

## 2. On-Policy vs Off-Policy

**Key Difference:**

<div class="pros-cons-grid" markdown="1">

<div class="pros" markdown="1">

#### ✓ On-Policy (SARSA, PPO, A3C)

- More stable training
- Simpler to implement
- Better for continuous control
- Direct policy improvement

**Trade-off:** Less sample efficient (can't reuse old data)

</div>

<div class="cons" markdown="1">

#### ⚡ Off-Policy (Q-Learning, DQN, DDPG)

- Highly sample efficient
- Can use replay buffers
- Reuses old experiences
- Better for discrete actions

**Trade-off:** Can be unstable, requires careful tuning

</div>

</div>

**Concrete example:**
```python
# Q-Learning (OFF-POLICY)
# Action taken: epsilon-greedy (exploratory)
action = epsilon_greedy_action(state)
next_state, reward = env.step(action)

# Update uses BEST action (greedy), not the one taken
Q[state, action] += alpha * (reward + gamma * max(Q[next_state]) - Q[state, action])
#                                              ↑ max = optimal, not what we'll actually do

# SARSA (ON-POLICY)
action = epsilon_greedy_action(state)
next_state, reward = env.step(action)
next_action = epsilon_greedy_action(next_state)  # What we'll ACTUALLY do next

Q[state, action] += alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])
#                                              ↑ uses actual next action from current policy
```

---

## 3. Q-Learning Formula & PyTorch

**Formula:**
```
Q(s, a) ← Q(s, a) + α[r + γ · max_a' Q(s', a') - Q(s, a)]
```
Where:
- s = current state, a = action taken
- r = reward received
- s' = next state
- α = learning rate
- γ = discount factor

**PyTorch Implementation:**
```python
import torch
import torch.nn as nn
import torch.optim as optim

class QNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )
    
    def forward(self, state):
        return self.net(state)

# Training step
q_network = QNetwork(state_dim=4, action_dim=2)
optimizer = optim.Adam(q_network.parameters(), lr=0.001)
gamma = 0.99

def train_step(state, action, reward, next_state, done):
    # Current Q-value
    q_values = q_network(state)
    current_q = q_values.gather(1, action.unsqueeze(1))
    
    # Target Q-value
    with torch.no_grad():
        next_q_values = q_network(next_state)
        max_next_q = next_q_values.max(1)[0]
        target_q = reward + gamma * max_next_q * (1 - done)
    
    # Loss and update
    loss = nn.MSELoss()(current_q.squeeze(), target_q)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

---

## 4. Policy Gradients - "But you always optimize parameters?"

**No! This is the key distinction:**

**Value-based methods (Q-learning):**
- Learn Q(s,a) values (critic)
- Policy is **implicit**: π(s) = argmax_a Q(s,a)
- Never directly optimize policy parameters
- Policy changes only when Q-values change

**Policy-based methods (Policy Gradients):**
- **Directly** parameterize policy: π_θ(a|s)
- Optimize θ to maximize expected return
- No Q-values needed (can be wasteful of data)

**Example:**
```python
# Policy network outputs action probabilities directly
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim),
            nn.Softmax(dim=-1)  # Direct probability output
        )
    
    def forward(self, state):
        return self.net(state)

# REINFORCE algorithm
def policy_gradient_update(states, actions, rewards):
    # Calculate returns (cumulative discounted rewards)
    returns = compute_returns(rewards, gamma=0.99)
    
    # Get action probabilities
    probs = policy_network(states)
    action_probs = probs.gather(1, actions.unsqueeze(1))
    
    # Policy gradient loss: -log(π(a|s)) * G_t
    loss = -(torch.log(action_probs) * returns).mean()
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

---

## 5. Actor-Critic - Is Critic a Separate Policy?

**No! Critic is NOT a policy.**

**Actor-Critic has TWO components:**

1. **Actor (Policy)**: π_θ(a|s) - decides what action to take
2. **Critic (Value function)**: V_φ(s) or Q_φ(s,a) - evaluates how good states/actions are

The critic doesn't choose actions; it just provides feedback to the actor.

**PyTorch Example:**
```python
class ActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        
        # Actor: outputs action probabilities
        self.actor = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim),
            nn.Softmax(dim=-1)
        )
        
        # Critic: outputs state value
        self.critic = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )
    
    def forward(self, state):
        action_probs = self.actor(state)
        state_value = self.critic(state)
        return action_probs, state_value

# Training
def actor_critic_update(state, action, reward, next_state, done):
    action_probs, value = model(state)
    _, next_value = model(next_state)
    
    # TD error (advantage)
    td_target = reward + gamma * next_value * (1 - done)
    advantage = td_target - value
    
    # Actor loss: policy gradient weighted by advantage
    action_prob = action_probs[action]
    actor_loss = -torch.log(action_prob) * advantage.detach()
    
    # Critic loss: TD error
    critic_loss = advantage.pow(2)
    
    total_loss = actor_loss + critic_loss
    total_loss.backward()
```

---

## 6. Is DPO an RL Method?

**Yes and No - It's RL without RL!**

**DPO (Direct Preference Optimization)** is designed to achieve the same goal as RLHF but without explicit RL training.

**Comparison:**

**RLHF (Traditional RL):**
1. Train reward model from preferences
2. Use PPO to optimize policy against reward model
3. Complex, unstable, requires careful tuning

**DPO:**
1. Directly optimizes policy from preferences
2. Closed-form solution derived from RL objective
3. Simpler, more stable, no reward model needed

**DPO Loss (derived from RL but computed directly):**
```python
def dpo_loss(policy_model, ref_model, preferred, rejected, beta=0.1):
    # Log probabilities from policy and reference model
    policy_pref_logprob = policy_model(preferred).log_prob()
    policy_rej_logprob = policy_model(rejected).log_prob()
    ref_pref_logprob = ref_model(preferred).log_prob()
    ref_rej_logprob = ref_model(rejected).log_prob()
    
    # DPO objective
    pref_ratio = policy_pref_logprob - ref_pref_logprob
    rej_ratio = policy_rej_logprob - ref_rej_logprob
    
    loss = -torch.log(torch.sigmoid(beta * (pref_ratio - rej_ratio)))
    return loss.mean()
```

**Answer**: DPO is RL-inspired but not RL in implementation. It's a supervised learning approach that provably optimizes the same objective as RL.

---

## 7. MDP

**MDP = Markov Decision Process**

The mathematical framework for RL:
- **S**: Set of states
- **A**: Set of actions
- **P**: Transition probability P(s'|s,a)
- **R**: Reward function R(s,a,s')
- **γ**: Discount factor

**Markov property**: Future depends only on current state, not history.

---

## 8. Value-Based vs Policy-Based Examples

**Value-Based (DQN):**
```python
class DQN:
    def __init__(self, state_dim, action_dim):
        self.q_net = QNetwork(state_dim, action_dim)
        self.target_net = QNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.q_net.parameters())
        self.replay_buffer = []
    
    def select_action(self, state, epsilon=0.1):
        if random.random() < epsilon:
            return random.randint(0, action_dim - 1)
        with torch.no_grad():
            q_values = self.q_net(state)
            return q_values.argmax().item()
    
    def train(self, batch_size=32):
        batch = random.sample(self.replay_buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        
        states = torch.stack(states)
        actions = torch.tensor(actions)
        rewards = torch.tensor(rewards)
        next_states = torch.stack(next_states)
        dones = torch.tensor(dones)
        
        # Current Q-values
        current_q = self.q_net(states).gather(1, actions.unsqueeze(1))
        
        # Target Q-values (using target network)
        with torch.no_grad():
            next_q = self.target_net(next_states).max(1)[0]
            target_q = rewards + 0.99 * next_q * (1 - dones)
        
        loss = nn.MSELoss()(current_q.squeeze(), target_q)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
```

**Policy-Based (REINFORCE):**
```python
class REINFORCE:
    def __init__(self, state_dim, action_dim):
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.policy.parameters())
        self.episode_data = []
    
    def select_action(self, state):
        probs = self.policy(state)
        dist = torch.distributions.Categorical(probs)
        action = dist.sample()
        self.episode_data.append({
            'state': state,
            'action': action,
            'log_prob': dist.log_prob(action)
        })
        return action.item()
    
    def train_episode(self, gamma=0.99):
        # Calculate returns (work backwards from end)
        returns = []
        G = 0
        for step in reversed(self.episode_data):
            G = step['reward'] + gamma * G
            returns.insert(0, G)
        
        returns = torch.tensor(returns)
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)  # Normalize
        
        # Policy gradient
        policy_loss = []
        for step, G in zip(self.episode_data, returns):
            policy_loss.append(-step['log_prob'] * G)
        
        loss = torch.stack(policy_loss).sum()
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        self.episode_data = []  # Clear for next episode
```

**Key Difference:**
- **DQN**: Learns Q(s,a), acts greedily based on Q-values
- **REINFORCE**: Directly learns π(a|s), samples from probability distribution

Great questions! Let me clarify these fundamental concepts:

## 1. Softmax Action Selection - Exploration Method

```python
def softmax_action(q_values, temperature=1.0):
    probs = torch.softmax(q_values / temperature, dim=0)
    return torch.multinomial(probs, 1).item()
```

**This IS exploration, but probabilistic, not random:**

- You're **sampling** from the probability distribution (not taking argmax)
- `torch.multinomial(probs, 1)` draws 1 sample based on probabilities
- Higher Q-value → higher probability, but not guaranteed

**Example:**
```python
q_values = torch.tensor([1.0, 2.0, 3.0])  # Q-values for 3 actions

# Temperature = 1.0
probs = softmax(q_values / 1.0)  # [0.09, 0.24, 0.67]
# Action 2 has 67% chance, but actions 0 and 1 still possible!

# Temperature = 0.1 (low = more greedy)
probs = softmax(q_values / 0.1)  # [0.0, 0.0, 1.0]
# Almost always picks action 2

# Temperature = 10.0 (high = more random)
probs = softmax(q_values / 10.0)  # [0.30, 0.33, 0.37]
# Nearly uniform exploration
```

**Contrast with epsilon-greedy:**
- Epsilon-greedy: 90% argmax, 10% completely random
- Softmax: Always probabilistic, never fully greedy or fully random

---

## 2. What Are Q-Values? (FUNDAMENTAL CONCEPT)

**Q(s, a) = Expected total future reward if you take action `a` in state `s`, then follow optimal policy**

Think of it as: "How good is this action in this situation?"

**Example - Pac-Man:**
```python
state = "ghost nearby on left, food on right"

Q[state, "move_left"] = -10   # Bad! Ghost will eat you
Q[state, "move_right"] = +50  # Good! Get food safely
Q[state, "move_up"] = +5      # Okay, safe but no food
Q[state, "move_down"] = +5    # Okay, safe but no food

# Policy: π(state) = argmax(Q[state, :]) = "move_right"
```

**q_values from neural network:**
```python
q_network = QNetwork(state_dim=4, action_dim=2)
state = torch.tensor([0.5, 0.3, -0.1, 0.2])

q_values = q_network(state)  # Output: tensor([2.3, 5.7])
# q_values[0] = 2.3  → Expected return for action 0
# q_values[1] = 5.7  → Expected return for action 1

best_action = torch.argmax(q_values).item()  # = 1 (higher Q-value)
```

---

## 3. Why Update Q[state, action]?

**Q is a table (or neural network) that stores Q-values for ALL state-action pairs:**

```python
# Tabular Q-learning (small discrete environments)
Q = {}  # Dictionary: Q[(state, action)] = value

# Example: GridWorld
Q[(0, 0), "right"] = 0.5
Q[(0, 0), "down"] = 0.3
Q[(0, 1), "right"] = 0.8
# etc...

# Update rule:
state = (0, 0)
action = "right"
reward = 1.0
next_state = (0, 1)

# Old Q-value for this state-action pair
old_q = Q[state, action]  # 0.5

# New target based on experience
target = reward + gamma * max(Q[next_state, a] for a in actions)
# target = 1.0 + 0.9 * 0.8 = 1.72

# Update this specific Q-value
Q[state, action] += alpha * (target - old_q)
Q[(0, 0), "right"] = 0.5 + 0.1 * (1.72 - 0.5) = 0.622
```

**You update BOTH state AND action because:**
- Q-values are specific to state-action PAIRS
- You need to know "which Q-value to update"
- It's like updating a specific cell in a spreadsheet

---

## 4. What Does .gather() Do?

**Problem**: Neural network outputs Q-values for ALL actions, but we only care about the action we took.

```python
# Network output: Q-values for all actions
q_values = q_network(state)  # tensor([[2.3, 5.7, 1.2, 4.0]])
#                              Action:    0    1    2    3

# We took action 1, so we only want q_values[1] = 5.7
action = torch.tensor([1])

# .gather() extracts the Q-value for the action we took
current_q = q_values.gather(1, action.unsqueeze(1))
# Result: tensor([[5.7]])
```

**Batch example:**
```python
# Batch of 3 experiences
q_values = torch.tensor([[2.3, 5.7, 1.2],   # Sample 0
                         [3.1, 2.0, 4.5],   # Sample 1  
                         [1.8, 6.2, 3.3]])  # Sample 2

actions = torch.tensor([[1],   # Sample 0 took action 1
                        [2],   # Sample 1 took action 2
                        [1]])  # Sample 2 took action 1

current_q = q_values.gather(1, actions)
# Result: tensor([[5.7],  # q_values[0, 1]
#                 [4.5],  # q_values[1, 2]
#                 [6.2]]) # q_values[2, 1]
```

---

## 5. Why Q-Values? Why Not Direct Policy?

**Excellent question! This is the key insight:**

### Value-Based (Q-Learning) Approach:

**Advantage**: Sample efficiency with replay buffer

```python
# Experience at timestep 1000
experience = (state, action, reward, next_state)
replay_buffer.add(experience)

# Timestep 5000: Policy has changed a lot, but we can STILL learn from old experience!
old_experiences = replay_buffer.sample(32)
for state, action, reward, next_state in old_experiences:
    # This still teaches us: "action X in state S led to reward R"
    # Q-values are universal truths about the environment
    q_target = reward + gamma * max(Q(next_state))
    q_loss = (Q(state, action) - q_target)^2
```

**Why Q-values allow this:**
- Q(s,a) is the "ground truth" about the environment
- Doesn't matter which policy generated the experience
- Old data remains valid (off-policy learning)

### Policy-Based (REINFORCE) Approach:

**Problem**: Can't reuse old data

```python
# With old policy π_old
action = π_old(state)  # prob = 0.1 for action A
reward = +10

# Much later, policy changed to π_new  
# π_new(state, action A) = 0.9  (now we LOVE action A)

# Can we use the old experience?
# NO! The gradient depends on current policy probabilities
# loss = -log(π_current(action)) * reward
# π_current is different now, so old gradients are wrong!
```

**When to use which:**

<div class="pros-cons-grid" markdown="1">

<div class="pros" markdown="1">

#### ✓ Value-Based (Q-Learning, DQN)

- Sample efficient with replay buffer
- Off-policy learning capability
- Good for discrete action spaces
- Deterministic policies

**Best for:** Data-scarce environments, robotics, expensive simulations

</div>

<div class="cons" markdown="1">

#### ⚡ Policy-Based (REINFORCE, PPO)

- Handles continuous actions naturally
- Can learn stochastic policies
- Better for high-dimensional actions
- Direct policy optimization

**Best for:** Continuous control, high-dimensional action spaces, stochastic policies

</div>

</div>

---

## 6. TD = Temporal Difference

**TD Error = Difference between predicted and actual (experienced) value**

```python
# Critic predicts state value
predicted_value = V(state)  # = 10

# Agent experiences: reward + next state value
actual_return = reward + gamma * V(next_state)  # = 5 + 0.9 * 8 = 12.2

# TD Error (advantage)
td_error = actual_return - predicted_value  # = 12.2 - 10 = 2.2
# "We did 2.2 better than expected!"
```

**Why called "Temporal Difference"?**
- Compares values at different TIME steps (t vs t+1)
- Uses bootstrapping: estimate from another estimate

**Three learning paradigms:**

1. **Monte Carlo**: Wait until episode ends, use actual total return
2. **TD Learning**: Update immediately using estimated next value
3. **n-step TD**: Compromise between the two

---

## 7. MDP vs Non-MDP Tasks

### MDP (Markov Decision Process) - Current state is enough:

✅ **Chess**: Board position tells you everything
```python
state = current_board_position
# Don't need to know how you got there
```

✅ **Atari with frame stacking**: Last 4 frames show velocity/direction
```python
state = [frame_t-3, frame_t-2, frame_t-1, frame_t]
# Contains motion information
```

✅ **Robot arm position**: Joint angles + velocities
```python
state = [θ1, θ2, θ3, θ̇1, θ̇2, θ̇3]
# Full information about arm state
```

### Non-MDP (POMDP - Partially Observable MDP):

❌ **Poker**: Can't see opponent's cards
```python
state = your_hand  # Not enough! 
# Need to remember betting history to infer opponent's cards
```

❌ **Atari with single frame**: Can't see ball direction
```python
state = single_frame_t
# Ball could be moving up or down - ambiguous!
```

❌ **Maze with limited vision**: Can't see full map
```python
state = "3 walls around you"
# Which room are you in? Need memory of path taken
```

**Solutions for non-MDP:**

<div class="todo-block">
<p><strong>Note:</strong> For POMDP (partially observable) environments, use memory mechanisms: LSTM/Transformers, frame stacking, or belief state tracking</p>
</div>

---

## 8. Is REINFORCE "Classic Learning"?

**No! REINFORCE is specifically RL, not supervised learning.**

### Supervised Learning (Classic):
```python
# You have labels
X = [image1, image2, ...]
Y = [cat, dog, ...]

loss = CrossEntropy(model(X), Y)  # Compare to ground truth
```

### REINFORCE (Policy Gradient RL):
```python
# No labels! Only rewards after taking actions
action = policy(state)  # Sample action
reward = environment.step(action)  # Try it!

loss = -log(policy(action|state)) * reward  # Higher reward → reinforce this action
```

**Comparison table:**

<div class="table-responsive">

| Aspect | Supervised | REINFORCE |
|--------|-----------|-----------|
| Labels | Yes (X→Y) | No, only rewards |
| Feedback | Immediate | Delayed |
| Data | Fixed dataset | Agent collects data |
| Optimization | Minimize error | Maximize reward |

</div>

**The "classic" RL algorithms are:**
- Q-Learning (1989)
- SARSA (1994)  
- Actor-Critic (1980s)
- REINFORCE (1992)

All are different from supervised learning!

# Q-Value Representations & Policy Gradient Explanation

## 1. Forms of Q-Values

Q can be represented in **many forms** depending on your problem:

### A. **Tabular Q (Dictionary/Table)**

For small, discrete state/action spaces:

```python
# Simple grid world (10x10 grid, 4 actions)
Q = {}  # Dictionary

# Initialize
for x in range(10):
    for y in range(10):
        for action in ['up', 'down', 'left', 'right']:
            Q[(x, y), action] = 0.0

# Access
state = (3, 5)
action = 'right'
q_value = Q[state, action]

# Update
Q[state, action] += 0.1 * (reward + gamma * max_next_q - Q[state, action])
```

**When to use:**
- Small state space (< 10,000 states)
- Discrete states and actions
- Example: Tic-tac-toe, small gridworlds

### B. **Neural Network Q (Function Approximation)**

For large/continuous state spaces:

```python
class QNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim)  # One Q-value per action
        )
    
    def forward(self, state):
        return self.network(state)

# Usage
q_net = QNetwork(state_dim=84*84*4, action_dim=18)  # Atari
state = preprocess(screen)
q_values = q_net(state)  # tensor([2.3, -1.1, 5.7, ...])
```

**When to use:**
- Large/continuous state space
- Images, high-dimensional sensors
- Example: Atari games, robotics

### C. **Linear Function Approximation**

Simple weighted combination of features:

```python
# Extract features from state
def extract_features(state):
    x, y, has_key, health = state
    return np.array([
        x, y,                    # Position
        x * y,                   # Interaction
        has_key,                 # Boolean feature
        health / 100.0,          # Normalized
        1.0                      # Bias term
    ])

# Q as linear function
class LinearQ:
    def __init__(self, num_features, num_actions):
        self.weights = np.zeros((num_actions, num_features))
    
    def get_q_value(self, state, action):
        features = extract_features(state)
        return np.dot(self.weights[action], features)
    
    def update(self, state, action, td_error):
        features = extract_features(state)
        self.weights[action] += alpha * td_error * features
```

**When to use:**
- Medium complexity
- You can engineer good features
- Fast training needed

### D. **Hand-Crafted/Rule-Based Q**

For domains with known heuristics:

```python
def handcrafted_q(state, action):
    """Q-values based on domain knowledge"""
    x, y, ghost_positions, food_positions = state
    
    if action == 'move_towards_ghost':
        return -100  # Very bad!
    
    if action == 'move_towards_nearest_food':
        distance = min(manhattan_distance((x, y), food) for food in food_positions)
        return 10 / (distance + 1)  # Closer food = higher Q
    
    if action == 'move_to_power_pellet':
        return 50  # High value
    
    return 0

# Use as initialization or baseline
q_network.load_state_dict(initialize_from_heuristics(handcrafted_q))
```

**When to use:**
- You have expert knowledge
- Warm-start learning
- Example: Game AI with human strategies

### E. **Hybrid: Dueling Network Architecture**

Separate value and advantage streams:

```python
class DuelingQNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.feature = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU()
        )
        
        # Value stream: V(s) - "how good is this state?"
        self.value_stream = nn.Sequential(
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )
        
        # Advantage stream: A(s,a) - "how much better is action a?"
        self.advantage_stream = nn.Sequential(
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )
    
    def forward(self, state):
        features = self.feature(state)
        value = self.value_stream(features)
        advantage = self.advantage_stream(features)
        
        # Q(s,a) = V(s) + (A(s,a) - mean(A(s,:)))
        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))
        return q_values
```

### F. **Cached/Memoized Q**

For expensive-to-compute Q-values:

```python
from functools import lru_cache

class CachedQ:
    def __init__(self):
        self.cache = {}
    
    @lru_cache(maxsize=10000)
    def get_q_value(self, state_tuple, action):
        """Cache frequently accessed Q-values"""
        if (state_tuple, action) in self.cache:
            return self.cache[(state_tuple, action)]
        
        # Expensive computation
        q_value = expensive_q_computation(state_tuple, action)
        self.cache[(state_tuple, action)] = q_value
        return q_value
```

---

## 2. Policy Does NOT Predict Next Action (in REINFORCE)

**Critical misunderstanding to clear up:**

### Policy GENERATES actions probabilistically:

```python
# Policy outputs probability distribution
policy_network(state) → [0.1, 0.7, 0.2]  # Probabilities for 3 actions
                         ↓    ↓    ↓
                      action0 action1 action2

# Then we SAMPLE from this distribution
action = sample([0, 1, 2], probs=[0.1, 0.7, 0.2])
# Might get: 1 (70% chance), or 0 (10% chance), or 2 (20% chance)
```

**It's stochastic, not deterministic prediction!**

---

## 3. Why `-log(prob) * reward`? (REINFORCE Explained)

This is the **core of policy gradient learning**. Let me break it down step by step:

### The Goal: Maximize Expected Reward

We want to find policy parameters θ that maximize:

```
J(θ) = E[Total Reward]
```

To maximize, we do gradient ascent:

```
θ ← θ + α * ∇J(θ)
```

### The Math (Simplified)

The gradient turns out to be:

```
∇J(θ) = E[∇log(π_θ(a|s)) * G]
```

Where:
- `π_θ(a|s)` = probability of action a in state s under policy θ
- `G` = total return (cumulative reward from that point)

**Taking the log makes the math work out** (from probability chain rule).

### Intuitive Explanation

```python
# Episode experience
state = [0.5, 0.3]
action_probs = policy(state)  # [0.1, 0.3, 0.6] for actions [0, 1, 2]

# We sampled action 1 (prob = 0.3)
action = 1
reward_from_this_episode = 10  # Good outcome!

# REINFORCE update
loss = -log(0.3) * 10
```

**What this does:**

1. **`-log(0.3)`** ≈ 1.2
   - If probability was high (0.9): `-log(0.9)` ≈ 0.1 (small gradient)
   - If probability was low (0.1): `-log(0.1)` ≈ 2.3 (large gradient)

2. **Multiply by reward (10)**:
   - Positive reward → increase probability of this action
   - Negative reward → decrease probability of this action

3. **Negative sign**:
   - We're minimizing loss, but maximizing reward
   - Negative flips it: minimizing `-reward` = maximizing reward

### Complete Example

```python
import torch
import torch.nn as nn
from torch.distributions import Categorical

class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim),
            nn.Softmax(dim=-1)
        )
    
    def forward(self, state):
        return self.net(state)

policy = PolicyNetwork(state_dim=4, action_dim=3)
optimizer = torch.optim.Adam(policy.parameters(), lr=0.01)

# Episode 1
states = []
actions = []
rewards = []

for t in range(100):  # 100 timesteps
    action_probs = policy(state)
    dist = Categorical(action_probs)
    action = dist.sample()
    
    next_state, reward, done = env.step(action)
    
    states.append(state)
    actions.append(action)
    rewards.append(reward)
    
    state = next_state
    if done:
        break

# Calculate returns (cumulative future rewards)
returns = []
G = 0
for r in reversed(rewards):
    G = r + 0.99 * G
    returns.insert(0, G)

returns = torch.tensor(returns)

# REINFORCE update
policy_loss = []
for state, action, G in zip(states, actions, returns):
    action_probs = policy(state)
    dist = Categorical(action_probs)
    
    # This is the key line!
    log_prob = dist.log_prob(action)  # log(π(a|s))
    policy_loss.append(-log_prob * G)  # -log(π) * return

loss = torch.stack(policy_loss).sum()
optimizer.zero_grad()
loss.backward()
optimizer.step()
```

### Why This Works - Intuitive Analogy

Imagine teaching a dog tricks:

**Scenario 1: Dog sits (action), gets treat (reward=+10)**
```python
prob_of_sit = 0.3  # Dog doesn't sit often
loss = -log(0.3) * 10 = -12

# Gradient descent on negative loss → increases prob_of_sit
# Next time: prob_of_sit becomes 0.5 (reinforced!)
```

**Scenario 2: Dog jumps on couch (action), gets scolded (reward=-5)**
```python
prob_of_jump = 0.8  # Dog jumps often
loss = -log(0.8) * (-5) = +1.1

# Gradient descent on positive loss → decreases prob_of_jump
# Next time: prob_of_jump becomes 0.6 (discouraged!)
```

### Why Log?

**Mathematical reason**: Makes gradient proportional to 1/probability

```python
d/dθ log(π(a|s)) = (1/π) * dπ/dθ
```

This creates a **natural learning rate**:
- Rare actions (low prob): Large gradient → learn faster
- Common actions (high prob): Small gradient → learn slower

**Practical reason**: Converts products to sums (numerical stability)

```python
# Without log (numerically unstable):
π(a1|s1) * π(a2|s2) * π(a3|s3) = 0.001 * 0.002 * 0.003 = 0.000000006

# With log (stable):
log(π(a1|s1)) + log(π(a2|s2)) + log(π(a3|s3)) = -6.9 + -6.2 + -5.8 = -18.9
```

---

## Summary Table

<div class="table-responsive">

| Q Representation | State Space | Example |
|------------------|-------------|---------|
| **Tabular** | Small, discrete | Tic-tac-toe |
| **Neural Net** | Large, continuous | Atari, Robotics |
| **Linear** | Medium, feature-based | Simple control |
| **Hand-crafted** | Domain expertise | Chess openings |
| **Dueling** | Complex decisions | Strategic games |
| **Cached** | Repeated states | Puzzle games |

</div>

**Key Insight**: Q is just a way to **store** "how good is action a in state s?" - the representation depends on your problem!